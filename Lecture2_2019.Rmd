---
title: "Lecture 2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```


#### Axioms of Probability
Kolmogorov's Axioms of Probability:


\vfill

\vfill

\vfill

\vfill
The textbook discusses the idea of belief functions. 
\vfill

#### Partitions and Bayes Rule
A collection of sets $\{H_1, ..., H_K\}$ is a **partition** of another set $\mathcal{H}$ if 

\vfill

\vfill

Let $\mathcal{H}$ be a Bozemanite's favorite ski hill. Partitions include:

\vfill
\vfill

Let $\mathcal{H}$ be the number of stats courses taken by people in this room. Partitions include:

\vfill
\vfill

\newpage
Suppose $\{H_1,...,H_K\}$ is a partition of $\mathcal{H}$, $Pr(\mathcal{H})=1$, and $E$ is some specific event. The axioms of probability imply the following statements:

1.  **Rule of Total Probability:** 
\vfill
2. **Rule of Marginal Probability:**

\vfill
3. **Bayes rule:**

\vfill

Assume a sample of MSU students are polled on their skiing behavior. Let \{$H_1, H_2, H_3, H_4$\} be the events that a randomly selected student in this sample is in, the first quartile, second quartile, third quartile and 4th quartile in terms of number of hours spent skiing.
\vfill

Then \{$Pr(H_1),Pr(H_2),Pr(H_3),Pr(H_4)\} = \{.25,.25,.25,.25\}$. 

\vfill
Let E be the event that a person has a GPA greater than 3.0, where \{$Pr(E|H_1),Pr(E|H_2),Pr(E|H_3),Pr(E|H_4)\} = \{.40,.71,.55,.07\}$.

\newpage
Now compute the probability that a student with a GPA greater than 3.0 falls in each quartile for hours spent skiing : \{$Pr(H_1|E),Pr(H_2|E),Pr(H_3|E),Pr(H_4|E)\}$ 
\vfill
\vfill

\vfill

#### Independence
Two events $F$ and $G$ are conditionally independent given $H$ if 
\vfill

If $F$ and $G$ are conditionally independent given $H$ then 
\vfill

What is the relationship between $Pr(F|H)$ and $Pr(F|H \cap G)$ as well as $Pr(F|H)$ and $Pr(F|H \cap I)$?
- $F = $\{ you draw the jack of hearts \}
- $G = $\{ a mind reader claims you drew the jack of hearts \}
- $H = $\{ the mind reader has extrasensory perception \}
- $I = $\{ Andy is the mind reader \}
\vfill
\vfill

#### Random Variables
In Bayesian inference a random variable is defined as an unknown numerical quantity about which we make probability statements. 
\vfill

##### Discrete Random Variables
Let $Y$ be a random variable and let $\mathcal{Y}$ be the set of all possible values of $Y.$ 
\vfill

The event that the outcome $Y$ of our study has the value $y$ is expressed as $\{Y=y\}$. 
\vfill
This function (known as the probability distribution function (pdf)) $p(y)$ has the following properties.

\vfill
\vfill

\newpage
###### Example 1. Binomial Distribution
Let $\mathcal{Y} = \{0, 1, 2, ..n\}$ for some positive integer $n$. Then $Y \in \mathcal{Y}$ has a binomial distribution with probability $\theta$ if

\vfill

###### Example 2. Poisson Distribution
Let $\mathcal{Y} = \{0,1,2...\}$. Then $Y \in \mathcal{Y}$ has a Poisson distribution with mean $\theta$ if 

\vfill

\subsubsection*{Continuous Random Variables}
Suppose that the sample space $\mathcal{Y}$ is $\mathbb{R},$ then $Pr(Y \leq 5) \neq \sum_{y \leq 5} p(y)$ as this sum does not make sense. Rather define the cumulative distribution function (cdf) $F(y) = Pr(Y \leq y)$.
\vfill
The cdf has the following properties:

\vfill
\vfill
\vfill

Using the CDF, probabilities of events can be derived as:

\vfill
\vfill

If F is continuous, then Y is a continuous random variable. Then $F(a) = \int_{-\infty}^a p(y) dy$.
\vfill

###### Example. Normal distribution.
Let $\mathcal{Y} = (-\infty, \infty)$ with mean $\mu$ and variance $\sigma^2$. Then y follows a normal distribution if 

\vfill
\newpage

#### Moments of Distributions
The mean or expectation of an unknown quantity $Y$ is given by

\vfill

The variance is a measure of the spread of the distribution.
\begin{eqnarray*}
	Var[Y] &=& E[(Y-E[Y])^2]\\
	&=& E[Y^2 - 2YE[Y] + E[Y]^2]\\
	&=& E[Y^2] - 2E[Y]^2 + E[Y]^2\\
	&=& E[Y^2] -E[Y]^2 
\end{eqnarray*}
\vfill

If $Y \sim$ Binomial$(n,p)$, then 
\vfill

if $Y \sim$ Poisson$(\mu)$, then 
\vfill

if $Y \sim$ Norma$l(\mu,\sigma^2)$, 
\vfill

#### Joint Distributions
Let $Y_1, Y_2$ be random variables, then the joint pdf or joint density can be written as

\vfill

The marginal density of $Y_1$ can be computed from the joint density:
\vfill
\vfill
Note this is for discrete random variables, but a similar derivation holds for continuous.

\vfill
The conditional density of $Y_2$ given $\{Y_1=y_1\}$ can be computed from the joint density and the marginal density.
\begin{eqnarray*}
p_{Y_2|Y_1}(y_2|y_1)& =& \frac{Pr(\{Y_1=y_1\} \cap \{Y_2=y_2\})}{Pr(Y_1=y_1)}\\
& =& \frac{p_{Y_1,Y_2}(y_1,y_2}{p_{Y_1}(y_1)}
\end{eqnarray*}
Note the subscripts are often dropped, so $p_{Y_1,Y_2}(y_1,y_2) = p(y_1,y_2)$, ect...
\vfill
\newpage

#### Independent Random Variables and Exchangeability

Suppose $Y_1, ..., Y_n$ are random variables and that $\theta$ is a parameter corresponding to the generation of the random variables. Then $Y_1, ..., Y_n$ are conditionally independent given $\theta$ if 
\vfill
\vfill

where $\{A_1, ..., A_n\}$ are sets. 
\vfill

Then the joint distribution can be factored as

\vfill
\vfill

If the random variables come from the same distribution then they are conditionally independent and identically distributed, which is noted $Y_1, ...., Y_n|\theta \sim i.i.d. p(y|\theta$ and
\begin{equation*}
p(y_1,...,y_n|\theta) = p_{Y_1}(y1|\theta) \times ... \times p_{Y_n}(y_n|\theta)= \prod_{i=1}^n p(y_i|\theta).
\end{equation*}
\vfill

#### Exchangeability
Let $p(y_1,...y_n)$ be the joint density of $Y_1, ..., Y_n$. If $p(y_1, ..., y_n) = p(y_{\pi_1}, ..., y_{\pi_n})$ for all permutations $\pi$ of \{1, 2, ...,n\}, then $Y_1, ..., Y_n$ are exchangeable.
\vfill

Assume data has been collected on apartment vacancies in Bozeman. Let $y_i = 1$ if an \emph{affordable} room is available. Do we expect $p(y_1 =0, y_2 =0, y_3=0, y_4 =1) = p(y_1=1, y_2=0, y_3 =0, y_4=0)$? If so the data are exchangeable.
\vfill


Let $\theta \sim p(\theta)$ and if $Y_1, ..., Y_n$ are conditionally i.i.d. given $\theta$, then marginally (unconditionally  on $\theta$) $Y_1, ..., Y_n$ are exchangeable. Proof omitted, see textbook for details.